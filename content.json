[{"title":"Spring Boot 启动速度优化","date":"2018-06-19T09:34:46.000Z","path":"2018/06/19/Spring Boot Start-up/","text":"Spring Boot 启动速度优化开始优化… 现用版本: java8; spring boot 1.5, gradle3.3; 监控工具: jProfiler9.2.1 参考文章: Spring Boot Performance –Alex Collins Configure a Spring Boot Web Application [TOC] 组件自动扫描问题@SpringBootApplication默认情况下, 我们会使用@SpringBootApplication注解来自动获取应用的配置信息. 这样会有一些副作用. 其中一个就是组件扫描@ComponentScan . 它会拖慢应用启动的速度, 也会加载一些不必要的bean. @EnableAutoConfiguration所以第一步就是干掉这两个注解, 使用EnableAutoConfiguration 来代替. 然后需要手动Import需要的class. 12345678910111213@EnableAutoConfiguration@Configuration@Import(value = &#123; DefaultConfiguration.class, BeforeControllerAdvice.class, DefaultSearchServiceImpl.class, SearchController.class,&#125;)public class DemoApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(DemoApplication.class, args); &#125;&#125; 如果不知道需要引入哪些类, 可以通过在启动命令中加入-Ddebug来打印出那些类是自动加载的. Spring Boot Performance)中提到的DispatcherServletAutoConfiguration等class, 不需要引入, 在项目启动时也会自动加载, 只需要Import自己写的类即可. 修改ServletTomcat默认情况下, Spring Boot使用Tomcat. Tomcat使用大约110MB的堆, 并且具有~16个线程. UndertowUndertow是一个用java编写的灵活的高性能Web服务器, 提供基于NIO的阻塞和非阻塞API. 相比于tomcat, undertow会占用更少的内存. 如何将servlet改为undertow12345compile(\"org.springframework.boot:spring-boot-starter-web:$&#123;springBootVersion&#125;\") &#123; exclude group: \"org.springframework.boot\", module: \"spring-boot-starter-tomcat\" &#125; compile group: \"org.springframework.boot\", name: \"spring-boot-starter-undertow\", version: \"$&#123;springBootVersion&#125;\" 若使用了spring-boot-starter-tomcat的其他工具类, 请自行引用相应工具类. -Xmx-Xmx参数是限制JVM的最大Heap, 若没有设置JVM会尽量多的吃更多的内存, 根据项目的实际需求设置Xmx可以避免java项目吃掉太多内存 java10最后, 也是最重要的一点, java10对java项目的启动做了优化, 将jdk版本升级到10以后, java的启动速度明显加快. 在java10, sping boot 2.0的环境下, 相比于java8, 快了20%以上(仅以一个项目做过测试, 数据仅供参考) 由于升级所带来的一些困扰, 不同的项目自行解决…","tags":[{"name":"spring boot","slug":"spring-boot","permalink":"http://localhost:4000/tags/spring-boot/"},{"name":"java","slug":"java","permalink":"http://localhost:4000/tags/java/"}]},{"title":"phpMyAdmin","date":"2018-06-11T03:00:20.000Z","path":"2018/06/11/phpMyAdmin/","text":"docker部署mysql&amp;phpMyAdmin记录如何使用docker部署mysql, 并通过myadmin管理, 不需要使用其他mysql工具 部署mysql123456789mysql: image: mysql:5.6.40 restart: always environment: - MYSQL_ROOT_PASSWORD=xxxxxxxx volumes: - /srv/mysql:/var/lib/mysql ports: - 3306:3306 这里需要指定mysql的密码, 并将/var/lib/mysql映射到主机上. 将3306端口映射到主机上, 以供其他服务连接. 部署phpmyadmin12345678910111213141516171819202122version: '2'services: mysql: image: mysql:5.6.40 restart: always environment: - MYSQL_ROOT_PASSWORD=xxxxxxxx volumes: - /srv/mysql:/var/lib/mysql ports: - 3306:3306 phpmyadmin: image: phpmyadmin/phpmyadmin:4.8.1 restart: always ports: - 33061:80 volumes: - /srv/session:/session environment: - PMA_HOST=mysql - PMA_USER=root - PMA_PASSWORD=xxxxxxxx 对于phpMyAdmin的详细信息可以查看https://github.com/phpmyadmin/docker, 如配置用户账号密码. 我是直接使用nginx的访问控制. 然后配置nginx就可以访问phpMyAdmin的页面, 对数据库进行操作. 连接数据库需要连接数据库是就可以用内网ip:3306来连接数据库. 如: 1SPRING_DATASOURCE_URL=jdbc:mysql://188.88.88.88:3306/database?characterEncoding=UTF-8&amp;useUnicode=true&amp;useJDBCCompliantTimezoneShift=true&amp;useLegacyDatetimeCode=false&amp;serverTimezone=Asia/Shanghai&amp;zeroDateTimeBehavior=convertToNull&amp;useSSL=false","tags":[{"name":"docker","slug":"docker","permalink":"http://localhost:4000/tags/docker/"},{"name":"mysql","slug":"mysql","permalink":"http://localhost:4000/tags/mysql/"}]},{"title":"Let's Encrypt 实现站点 SSL","date":"2018-04-16T08:45:28.000Z","path":"2018/04/16/Let’s Encrypt SSL/","text":"Let’s Encrypt 实现站点 SSL本文介绍如何使用Let’s Encrypt实现网站的https访问 引用了下列网站的内容: 免费 SSL：Ubuntu 16.04 配置 Let’s Encrypt 实现站点 SSL [使用Let’s Encrypt加密你的小站] 介绍Let’s Encrypt是一个免费并且开源的CA, 且已经获得Mozilla, 微软等主要浏览器厂商的根授信. 它极大低降低DV证书的入门门槛, 进而推进全网的HTTPS化. 安装安装Let’s Encrypt的自动部署脚本: Certbot. 123456789# 安装nginx, 若已安装, 可跳过apt-get install nginx# 添加certbot的package repositoryadd-apt-repository ppa:certbot/certbot# 提示 Press [ENTER] to continue or ctrl-c to cancel adding it, 直接Enter即可# 更新数据源apt-get update# 安装 certbot, 若机器上已有python和nginx, 可以只安装certbot. `apt-get install certbot`apt-get install python-certbot-nginx 签发SSL证书使用certbot签发证书 1certbot certonly --standalone --email your@email.com -d yourdomain.com -d test.yourdomain.com 证书会生成在/etc/letsencrypt/live/yourdomain下, 如有需要, 可自行copy到其他文件夹. 配置nginx在nginx目录下创建ssl-test.conf, 内容如下: 12345678910111213141516171819202122232425262728server &#123; # SSL configuration listen 443 ssl; listen [::]:443 ssl; ssl on; ssl_certificate /etc/letsencrypt/live/yourdomain.com/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/yourdomain.com/privkey.pem; ssl_session_timeout 5m; ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; root /var/www/html; # Add index.php to the list if you are using PHP index index.html index.htm index.nginx-debian.html; location / &#123; # First attempt to serve request as file, then # as directory, then fall back to displaying a 404. proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; try_files $uri $uri/ =404; &#125;&#125; 重新启动nginx服务 1nginx -t &amp;&amp; nginx -s reload 这时候在域名前加https就可以访问了. 添加https访问在nginx文件夹下新建一个ssl.conf, 统一配置, 暂时没有用 123456789101112131415161718192021222324252627ssl on;ssl_session_cache shared:SSL:10m;ssl_session_timeout 24h;ssl_buffer_size 1400;ssl_session_tickets off;ssl_protocols TLSv1 TLSv1.1 TLSv1.2;ssl_ciphers AES256+EECDH:AES256+EDH:!aNULL;# ssl_ciphers &quot;EECDH+AESGCM:EDH+AESGCM:ECDHE-RSA-AES128-GCM-SHA256:AES256+EECDH:DHE-RSA-AES128-GCM-SHA256:AES256+EDH:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-SHA384:ECDHE-RSA-AES128-SHA256:ECDHE-RSA-AES256-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES256-SHA256:DHE-RSA-AES128-SHA256:DHE-RSA-AES256-SHA:DHE-RSA-AES128-SHA:ECDHE-RSA-DES-CBC3-SHA:EDH-RSA-DES-CBC3-SHA:AES256-GCM-SHA384:AES128-GCM-SHA256:AES256-SHA256:AES128-SHA256:AES256-SHA:AES128-SHA:DES-CBC3-SHA:HIGH:!aNULL:!eNULL:!EXPORT:!DES:!MD5:!PSK:!RC4&quot;;ssl_prefer_server_ciphers on;ssl_certificate /etc/letsencrypt/live/yourdomain.com/fullchain.pem;ssl_certificate_key /etc/letsencrypt/live/yourdomain.com/privkey.pem;#ssl_stapling on;#ssl_stapling_verify on;#resolver 119.29.29.29 223.5.5.5 223.6.6.6 valid=600s;#resolver_timeout 30s;#spdy_keepalive_timeout 300;#spdy_headers_comp 9;#add_header Strict-Transport-Security max-age=63072000;#add_header Strict-Transport-Security &quot;max-age=31536000; includeSubdomains&quot; always;add_header X-Frame-Options DENY;add_header X-Content-Type-Options nosniff; 创建一个conf 123456789101112131415server &#123; listen 443 ssl; server_name test.yourdomain.com; ssl on; ssl_certificate /etc/letsencrypt/live/xiaoshaniu.xin/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/xiaoshaniu.xin/privkey.pem; client_max_body_size 1G; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; location / &#123; proxy_pass http://ip:port; &#125;&#125; 仅限https访问, 可以不加 123456server &#123; listen 80; server_name test.yourdomain.com; rewrite ^(.*) https://$server_name$1 permanent;&#125; 记得nginx reload 1nginx -t &amp;&amp; nginx -s reload 至此, 就可以https加密就完成了 自动更新证书因为Let’s Encrypt签发的SSL证书有效期只有90天, 所以我们需要在90天内更新证书. Let’s Encrypt也将自动更新的脚本添加到了/etc/cron.d里, 只需要执行下面的命令验证一下就可以了. 1certbot renew --dry-run","tags":[{"name":"SSL","slug":"SSL","permalink":"http://localhost:4000/tags/SSL/"},{"name":"HTTPS","slug":"HTTPS","permalink":"http://localhost:4000/tags/HTTPS/"}]},{"title":"kubernetes-istio","date":"2018-04-12T10:15:04.000Z","path":"2018/04/12/kubernetes-istio/","text":"kubernetes-istiokubernetes集成istio 原文地址: http://istio.doczh.cn/docs/setup/kubernetes/quick-start.html 基于现有的kubernetes集群安装istio(1.7.3 后更高版本, 并且启用了RBAC ) 介绍IstioEnvoyIstio核心组件, 包含了动态服务发现, 负载均衡, TLS终止, HTTP/2&amp;gRPC代理, 熔断器, 健康检查, 基于百分比流量拆分的分段推出, 故障注入和丰富指标等功能. Envoy被部署为sidecar, 和对应的服务在同一个pod中. Mixer 负责服务网格上执行访问控制和使用策略, 并从Envoy代理和其他服务收集遥测数据. Pilot 负责收集和验证配置并将其传播到各种Istio组件. Istio-Auth 提供强大的服务间认证和终端用户认证, 使用交互TLS, 内置身份和证书管理. 安装Istio下载最新版1curl -L https://git.io/getLatestIstio | sh - 或者手动下载 istio, 解压 下载最新版并解压, 需要手动将istioctl复制到PATH路径下, 或将 istioctl 客户端二进制文件加到 PATH 中. 进入istio文件夹后操作 安装 Istio1kubectl apply -f install/kubernetes/istio.yaml 注意：如果运行的集群不支持外部负载均衡器, istio-ingress 服务的 EXTERNAL-IP 显示&lt;pending&gt;. 必须改为使用 NodePort service 或者 端口转发方式来访问应用程序 验证安装12kubectl get svc -n istio-systemkubectl get pods -n istio-system pod状态都为Running即成功. 部署应用准备1234567891011apiVersion: config.istio.io/v1alpha2kind: EgressRulemetadata: name: mysql namespace: defaultspec: destination: service: MYSQL_IP ports: - port: 3306 protocol: tcp 1234567891011apiVersion: config.istio.io/v1alpha2kind: EgressRulemetadata: name: redis namespace: defaultspec: destination: service: REDIS_IP ports: - port: 6379 protocol: tcp 1234567891011apiVersion: config.istio.io/v1alpha2kind: EgressRulemetadata: name: es namespace: defaultspec: destination: service: ES_IP ports: - port: 9300 protocol: tcp 部署我们并没有启动istio-initializer, 所以需要手动注入Envoy 1kubectl create -f &lt;(istioctl kube-inject -f DEPLOYMENT.yaml) 若服务已创建需要使用kubectl apply 1kubectl apply -f &lt;(istioctl kube-inject -f DEPLOYMENT.yaml) 部署相应的service和ingress, 可先参考神坑, 避免一些重复的坑. 配套设施1. zipkin部署 1kubectl apply -f install/kubernetes/addons/zipkin.yaml 通过istio-ingress访问 创建ingress 123456789101112131415apiVersion: extensions/v1beta1kind: Ingressmetadata: name: zipkin namespace: istio-system annotations: kubernetes.io/ingress.class: \"istio\"spec: rules: - http: paths: - path: /zipkin/.* backend: serviceName: zipkin servicePort: 9411 访问 http://hostip:31100/zipkin/ 2. Prometheus部署 1kubectl apply -f install/kubernetes/addons/prometheus.yaml 通过nodePort访问 1kubectl -n istio-system edit service prometheus 访问 http://hostip:31701/graph 创建prometheus数据收集规则请参考: http://istio.doczh.cn/docs/tasks/telemetry/metrics-logs.html http://istio.doczh.cn/docs/tasks/telemetry/tcp-metrics.html 3. grafana部署 1kubectl apply -f install/kubernetes/addons/grafana.yaml 通过nodePort访问 1kubectl -n istio-system edit service grafana 访问 http://hostip:31702/ 4. servicegraphServicegraph服务是一个示例服务, 他提供了一个生成和展现Service Mesh中服务关系的功能, 包含如下几个服务端点: /graph: 提供了servicegraph的JSON序列化展示 /dotgraph: 提供了servicegraph的Dot序列化展示 /dotviz: 提供了servicegraph的可视化展示 所有的端点都可以使用一个可选参数time_horizon, 这个参数控制图形生成的时间跨度. 另外一个可选参数就是filter_empty=true, 在time_horizon所限定的时间段内, 这一参数可以限制只显示流量大于零的node和edge. Servicegraph示例构建在Prometheus查询之上. 安装 1kubectl apply -f install/kubernetes/addons/servicegraph.yaml 通过nodePort访问 1kubectl -n istio-system edit service servicegraph 访问 http://hostip:31703/dotviz 部署istio项目时遇到的坑 istio部署服务一直连不上数据库, 我不论用什么方法都没用, istio都删了重新装的不行, 原因是ip写错了… 要创建egress-role 1234567891011apiVersion: config.istio.io/v1alpha2kind: EgressRulemetadata: name: mysql namespace: defaultspec: destination: service: &lt;MySQL instance IP&gt; ports: - port: &lt;MySQL instance port&gt; protocol: tcp istio-ingress 404 path需要配置成/qingdaoliuyi/.*, 否则只能访问/qingdaoliuyi, 路径下的css, js会404. istio-ingress istio-ingress仅对service中name是http的ports起作用. 相关issue deployment部署后, 删除istio并重新部署, 更新deployment后egress无效?(待验证问题) 发送请求到外部HTTPS服务 外部的HTTPS服务必须可以通过HTTP访问，在请求中指定端口： 1curl http://www.google.com:443 最后, 由于egress对于https的支持还有一定的问题, 所以暂时没有将istio加进我们kubernetes里.","tags":[{"name":"笔记","slug":"笔记","permalink":"http://localhost:4000/tags/笔记/"}]},{"title":"维护者的最佳实践","date":"2018-01-30T03:12:16.000Z","path":"2018/01/30/Best Practices for Maintainers/","text":"维护者的最佳实践 原文地址: https://opensource.guide/best-practices/ 作为开源维护者, 通过记录流程到充分利用社区, 可以让你的生活更轻松 [TOC] 1. 成为一个维护者意味着什么如果你维护者一个大量用户使用, 你可能已经意识到你现在已经很少去编码, 而会花大量时间去回复issues. 在一个项目的初期, 你会去尝试许多新的点子, 并且根据你想要什么去做决定. 随着你的项目知名度越来越高, 你会发现你更多的时候是为你的使用者和贡献者工作. 维护一个项目需要的不仅仅是编码. 有些任务常常是出乎意料的, 但他们又对一个成长中的项目非常重要. 我们收集了一些方法是你的生活更加轻松, 从记录流程到充分利用社区资源 2. 记录你的流程把事情记录下来是作为一个维护者能够做的最重要的一件事情 文档不仅可以阐明你自己的事情, 也可以帮助其他人在询问你之前了解你需要或期望的东西. 写文档可以让你更容易的拒绝一些不适合项目的东西. 也可以让其他人更容易参与帮助你的项目. 你永远不可能知道谁在阅读或使用你的项目. 即使你不写出完整的文档, 记录下重点也比什么都不写好很多. 记住及时更新你的文档. 如果你不能一直这样做, 请删除过期的文档或指出已过时, 以便贡献者了解更新. 写下你的项目的愿景首先写下你的项目的目标. 把它们添加进README中, 或者创建一个名为VISION的单独的文件. 如果有其他需要帮助的部分, 例如项目流程图, 最后也公开这些东西. 拥有一个清晰的, 文档化的愿景可以让你专注, 并且帮助你避免他人的贡献中的’范围蔓延’. 例如, lord发现拥有一个项目愿景帮助他弄清楚哪些需求需要花费时间. 作为一个新的维护者, 他后悔当他接到第一个Slate功能需求的时候没有坚持自己的项目范围. 我摸索了它, 我没有努力的想出一个完整的解决方案. 我希望我曾经说过’我现在没有时间做这件事, 但我会把他加进长期的目标列表中’, 而不是一个半途而废的解决方法. —- @lord 沟通你的期望写下这些规则是很伤脑筋的. 有时候你可能觉得你在监督别人的行为或者扼杀所有的乐趣. 然而, 编写和执行良好的规则可以增强维护人员的能力. 它们阻止你去做你不想做的事情. 大部分看到你项目的人都不知道你和你的情况. 他们可能假设你得到报酬, 特别是他们经常使用和依赖的东西. 也许在某个时间, 你花费大量的时间在你的项目上, 但是现在你正忙于一份新的工作或者家庭成员. 所有的这一切都很完美! 只要确保其他人知道就行了. 如果维护你的项目是兼职或者纯粹资源的, 要诚实的告知别人你有多少时间. 这与你认为项目需要花费多少时间, 或者其他人想让你花费多少时间是不一样的. 这里有一些值得写下来的规则: 如何评估和接受贡献(是否需要测试?问题模板) 你接受什么类型的贡献(你是否只希望得到部分的代码帮助) 什么时候适合跟进(例如, 你可以期望维护人在7天之内回应, 如果你到那时还没有听到任何消息, 那就自由的去做吧) 你在这个项目上会花费多少时间(例如, 我们平均每个星期只花费5个小时在这个项目上) Jekyll, CocoaPods, 和Homebrew 是几个项目的例子, 它们为维护人员和贡献者提供了基本规则. 保持沟通公开也不要忘记及时记录你的沟通记录. 不论你再哪里, 都要保持你的项目公开, 如果有人想要私下和你联系讨论某个功能请求或需求支持, 礼貌的将他引导公共的沟通渠道中, 比如邮件列表或issue tracker(问题追踪器) 如果你想要和其他维护者会面, 或者私下做出一些重大决定, 在公开场合发布这些对话, 即使只是发布一些你的笔记. 这样, 任何加入你的社区的人都可以获得和那些已经在那里多年的人相同的信息. 3. 学会说NO你已经记录下了一些的事情. 理想情况下, 每个人都会阅读你的文档, 但实际上, 你必须提醒其他人, 这些知识是存在的 把一些都写下来, 不论如何, 它将在你需要执行你的规则的时候帮你个性化解决方案. 说’不’并不有趣, 但’我不喜欢你的贡献’不如’你的项目不符合这个项目的标准’. 说’不’适用于许多你会遇到的维护者: 那些不符合范围的特性请求, 有人破坏了讨论, 对其他人做了不必要的工作. 保持对话友好你要练习说’不’的最重要的地方之一就是你的issue和pull request. 作为一个项目维护者, 你将不可避免地收到你不想接受的建议. 也许这个贡献改变了你项目的范围或不符合你的愿景. 或者这个想法很好, 但是实现很糟糕. 无论什么原因, 都有可能巧妙地处理那些不符合项目标准的贡献. 如果你收到了一个你不想接受的贡献, 你的第一反应可能是忽略或者假装没有看到. 这样做会伤害别人的感情, 甚至会使你所在社区的其他潜在贡献者失去动力. 为大型开源项目提供支持的关键是保持问题的进展. 尽量避免出现问题. 如果你是一个iOS开发者, 你知道提交雷达是多么令人沮丧. 你可能会在两年后听到, 并被告知尝试使用最新版本的iOS —- @KrauseFx 不要因为内疚或想要友善而对你不想要的贡献置之不理. 随着时间的推移, 你没有回复的issues和PRs将使你的项目感到压力和威胁. 最好立即关闭你明确知道你不想接受的贡献. 如果你的项目已经遭受大量积压, @steveklabnik 对于如何有效的分流问题有一些建议. 其次, 忽略贡献会给你的社区带来一个消极的信号. 对一个项目作出贡献是令人生畏的, 特别是如果这是一些人的第一次. 即使你不接受他们的贡献, 之后也要承认他们, 并感谢他们的关心, 这是一个很大的赞扬! 如果你不想接受一个贡献: 感谢他们的贡献 解释为什么它不符合项目的范围, 并提供明确的改进建议. 如果可以, 尽量亲和一下，但要坚定. 如果你有文档, 链接到相关文档. 如果您注意到重复的请求, 请将它们添加到文档中, 以避免自己重复工作 关闭请求 你不应该需要超过1-2个句子来回应. 例如, 当celery用户报告与Windows相关的错误时， @berkerpeksag回应： 如果说’不’的想法吓到了你, 你并不孤单. 就像 @jessfraz 说的: 我和几个不同的开源项目, Mesos, Kubernetes, Chrominm谈过, 他们都认为最为维护者, 对于你不想要的不定说’No’是最困难的部分之一. 不要以为不想接受别人的贡献而感觉到内疚. 根据@shykes的说法, ‘No只是暂时的, Yes才是永久的’. 虽然同求别人的热情是一件好事, 但拒绝一个人的贡献和拒绝一个人的支持是不一样的. 最后, 如果一个贡献不够好, 你就没有义务去接受他. 当被人对你的项目作出贡献的时候, 你要表现的友善和积极, 但只接受你真正相信的修改会让你的项目更好. 你练习说’不’的次数越多, 它就变得越容易. 真的! 主动为了减少不必要的贡献, 你需要解释在你的贡献指南中说明你的项目提交和接受贡献的过程. 如果你收到了太多低质量的贡献, 则需要贡献者提前做一些工作, 例如: 填写issue或者PR的模板/清单 在PR之前打开一个issue 如果他们不遵循你的规则, 马上关闭这个issue并指向你的文档. 虽然这种方法一开始可能有些不友好, 但积极主动实际上对双方都有好处. 它减少了有人将大量浪费的工作时间投入到你不愿意接受的请求. 而且它会让你的工作量更容易管理. 理想情况下，向他们说明并且在CONTRIBUTING.md文件解释他们将来如何在开始工作之前得到更好的指示 —- @mikemcquaid 有时候, 当你说’不’时, 你的潜在贡献者可能会感到不安或批评你的决定. 如果他们的行为变得敌对, 采取措施缓和局势, 或者甚至把他们从你的社区中移除, 如果他们不愿意建设性地合作. 接受指导也许你的社区中的一些人会经常提交一些不符合你项目标准的贡献. 反复的拒绝会让双方都很沮丧. 如果你看到一些人对你的项目很热情, 但需要一些磨砺, 你要有耐心. 在每个情况下清楚地解释为什么他们的贡献不符合项目的期望. 试着将它们指向一个更容易或更不明确的任务, 比如标记一个问题为’很好的第一个BUG’, 让他们初次尝试. 如果你有时间, 可以考虑通过他们的第一个贡献来指导他们, 或者在你的社区里找到一个愿意指导他们的人. 4. 利用你的社区你不必自己做一切. 你项目的社区存在是有原因的! 即使你还没有一个积极的贡献者社区, 如果你有很多的用户, 让他们工作. 分担工作量如果您正在寻找其他人参与, 首先从四周询问. 当你看到新的贡献者作出重复贡献时, 通过提供更多的责任来认识他们的工作. 如果他们愿意, 记录下他们如何成长为一个领导者. 鼓励他人分享项目的所有权可以很好的减少你自己的工作量, 就像 @lmccart在她的p5.js项目中发现的那样. 我曾说过, ‘是的, 每个人都可以参与, 你不需要拥有很多的编码知识[…]’. 我们让人们来参加(一项活动), 当时我真的很想知道: 这是真的, 我一直在说什么? 会有40个人出现, 而不是我可以和他们每个人坐在一起…但是人们聚在一起, 而且这种工作方式很有效. 一旦有人获得了, 他们就可以教导他们的邻座. I’d been saying, “Yeah, anyone can be involved, you don’t have to have a lot of coding expertise […].” We had people sign up to come [to an event] and that’s when I was really wondering: is this true, what I’ve been saying? There are gonna be 40 people who show up, and it’s not like I can sit with each of them…But people came together, and it just sort of worked. As soon as one person got it, they could teach their neighbor. 如果你需要离开你的项目, 不论暂时的还是永久的, 请求别人接管你并不是一件难以启齿的事情. 如果其他人热衷于它的方向, 给予他们提交的权限或这正式的将控制权转交个某个人. 如果有人fork了你的项目, 并且在非常积极的维护它, 考虑从你原始的项目链接到这个fork的项目. 这对那些希望你的项目维护下去的人是很重要的. @progrium 发现, 记录他的项目Dokku的愿景, 使得即使他离开了这个项目, 这些目标仍然有人继续下去: 我写了一个wiki描述我的目标和为什么我制定这些目标. 因为一些某些原因, 让我很惊讶的是, 项目的维护者们的开发方向都开始往这个方向偏移. 刚好我做了这些这一切就发生了? 不一定. 但它的确让我的项目跟接近我写下的方向. I wrote a wiki page describing what I wanted and why I wanted it. For some reason it came as a surprise to me that the maintainers started moving the project in that direction! Did it happen exactly how I’d do it? Not always. But it still brought the project closer to what I wrote down. 让其他人构建他们需要的解决方案如果有一个潜在的贡献者对于你的项目应该怎么做有不同的意见, 你可能需要慢慢的鼓励他们在他们自己fork的项目里操作. Fork一个项目不一定是一件坏事. 能够复制和修改项目是关于开源的最好的事情之一. 鼓励你的社区成员自己工作, 可以提供他们需要的创造性的出路, 而不会与你的项目的愿景相冲突. 我迎合80%的用例. 如果你是独角兽之一, 请for我的项目. 我不会生气的! 我的公共项目大部分是要解决公共的问题; 我试图通过分工或扩大工作来让事情变得更加容易. ​ —- @geerlingguy, “Why I Close PRs” 同样的道理也适用于那些真正想要解决方案的用户, 因为他们根本没有足够的带宽来构建. 提供一些API和定制的hooks可以帮助其他人满足他们自己的需求, 而不需要直接修改源码. @orta 发现鼓励CocoaPods导致了一些”有趣的想法”: 几乎是不可避免的, 当一个项目变大, 维护者们必须对于如何采用新代码更加谨慎. 你变得擅长说’NO’, 但很多人都有合理的需求. 所以, 反而你最终将你的工具转换成一个平台. 5.带上机器人就像其他人帮你做的事情一样, 也有一些不需要人来做的事情. 机器人就是你的朋友. 使用它们可以使你作为一个维护者的生活更容易. 需要测试和其他的检查来提高你的代码质量可以使得的项目自动化最重要的一个方法就是添加测试. 测试可以使贡献者感到自行, 因为它们不会破坏任何事情. 同时也可以让你快速审查和接受代码变得更容易. 你的反应速度越快, 你的社区就越容易参与. 设置当贡献者提交代码是跑的自动测试, 并确认你的测试可以让贡献者很轻易的在本地跑起来. 要求所有的代码贡献者在他们提交之前必须通过你的测试. 你将帮助设置一个提交的最低质量标准. Gitlab的状态检查可以帮助你确认没有通过测试的代码不会被合并. 如果你添加测试, 确保在CONTRIBUTING文件中解释测试如何运行. 我相信在人开发的代码中测试是必须的. 如果代码是完全正确的, 它就不需要被修改–我们只在一些事错误的时候才写代码, 不论是”它崩溃了”或”它没有这样的功能”. 无论你作出什么改变, 测试对于捕捉任何回归或你意外引入的问题都是必须的. —- @edunham, “Rust’s Community Automation” 使用工具自动执行一些基本的维护任务关于维护一个受欢迎的项目的好消息是, 有其他的维护者面临过同样的问题, 并构建了一个解决方法. 有各种工具可以帮助自动化维护工作的某些方面, 一些例子: semantic-release 自动发布版本 mention-bot 提到对PR的潜在审查 Danger 帮助自动化代码审查 对于bug报告和其他常见的贡献, GitHub有issue模板和PR模板, 你可以创建模板来帮你简化收到的消息. @TalAter建了一个选择你自己的冒险指南来帮助你创建自己的issue模板和PR模板. 为了管理你的邮件提醒, 你可以设置邮件过滤来根据优先级管理. 如果你想要更先进一点, 风格指南和linters可以使项目贡献规范化, 并且使得它们更容易审查和接受. 无论如何, 如果你的规范太复杂, 它们会增加贡献的难度. 确保你只是增加了足够的规则, 让每个人的生活更轻松. 如果你不确定使用哪些工具, 参考一起其他受欢迎的项目怎么做, 特别是与你类似的醒目. 例如, 其他Node模块的贡献是什么样的? 使用类似的工具和方法也会让你的贡献者更加熟悉你的流程. 6.可以暂停开源项目曾经使你快乐, 也许现在它开始让你感到回避或内疚. 也许当你想到你的项目时, 你会感到不堪重负或者越来越恐惧, 与此同时, issuses和PRs也堆积如山. 倦怠是开源工作中一个普遍的问题, 尤其是在维护者中. 作为 一个维护者, 你的幸福对于任何开源项目的生存都是没有商量余地的. 虽然不用说, 休息一下! 你不应该等到感到脑袋都要烧起来才去休息. @brettcannon, 一个Pathon的核心开发者, 决定在14年的自愿开源软件开发工作后进行为期一个月的假期. 就像其他任何类型的工作, 定期的休息会让你头脑清晰, 对你工作更开心, 兴奋. 在维护WP-CLI过程中, 我发现我需要是我自己开心, 并为我的参与设置一个清晰的界限. 最好的平衡点是每周2-5小时, 作为我日常工作表的一部分. 这使我的参与变得充满激情, 并感觉很像一份工作. 因为我优先考录我正在处理的问题, 我可以在我认为最重要的事情上定期取的进展. ​ —- @danielbachhuber, “My condolences, you’re now the maintainer of a popular open source project” 有时, 当感觉所有人都需要你时, 你很难重开源项目中解脱出来. 人们甚至会试图让你因为离开而感到内疚. 当你离开一个项目时, 尽最大努力为你的用户和社区寻找支持. 如果你不能找到你需要的支持, 那就休息一下. 你休息时记得保持联系, 这样人们就不会因为你缺乏响应而感到困惑. 休息也适用于休假. 如果你不想周末或者工作时间做开源项目的工作, 把这些期望告知别人, 这样别人知道在什么时候不打扰你. 7.先照顾好自己!维护一个受欢迎的项目不同于早起增长技能阶段, 它没有什么回报. 作为一个维护者, 你将在很少人能够体验到的水平上实践领导能力和个人技能. 虽然管理起来不容易, 但要设定清晰的界限, 并且只关心你觉得舒服的事, 会使你保持快乐, 精力充沛和富有成效.","tags":[{"name":"翻译","slug":"翻译","permalink":"http://localhost:4000/tags/翻译/"}]},{"title":"Kubernetes安装脚本(docker-compose)","date":"2018-01-25T06:17:17.000Z","path":"2018/01/25/kubenetes-docker/","text":"Kubernetes安装脚本环境节点列表 NODE1：127.0.0.1 NODE2：127.0.0.2 NODE3：127.0.0.3 Docker daemon config12345678910cat &gt;/etc/docker/daemon.json &lt;&lt;EOF&#123; \"registry-mirrors\": [\"https://1.mirror.aliyuncs.com\"], \"log-driver\": \"json-file\", \"log-opts\": &#123; \"max-size\": \"10m\" &#125;, \"bridge\": \"docker0\"&#125;EOF Install cfssl12345# Install cfssl utils for generating certs.for bin in cfssl cfssljson cfssl-certinfo; do curl -sSL -o /usr/local/bin/$&#123;bin&#125; https://pkg.cfssl.org/R1.2/$&#123;bin&#125;_linux-amd64 chmod +x /usr/local/bin/$&#123;bin&#125;done Generate config123456789101112131415cat &gt;ca-config.json &lt;&lt;EOF&#123; \"signing\": &#123; \"default\": &#123; \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"876000h\" &#125; &#125;&#125;EOF Set Env12345678910111213141516export NODE1=\"127.0.0.1\"export NODE2=\"127.0.0.2\"export NODE3=\"127.0.0.3\"export MASTER_IPS=\"$&#123;NODE1&#125;,$&#123;NODE2&#125;,$&#123;NODE3&#125;\"export ETCD_INITIAL_CLUSTER=\"etcd-01=https://$&#123;NODE1&#125;:2380,etcd-02=https://$&#123;NODE2&#125;:2380,etcd-03=https://$&#123;NODE3&#125;:2380\"export ETCD_SERVERS=\"https://$&#123;NODE1&#125;:2379,https://$&#123;NODE2&#125;:2379,https://$&#123;NODE3&#125;:2379\"export ETCD_INITIAL_CLUSTER_TOKEN='myzd-prod-etcd-cluster-token'export KUBE_API_IP=$&#123;NODE1&#125;export KUBE_API_CLUSTER_IP=\"172.16.0.1\"export CLUSTER_CIDR=\"172.16.64.0/18\"export SERVICE_CLUSTER_CIDR=\"172.16.0.0/18\"export CLUSTER_DNS=\"172.16.0.3\"export KUBE_API_HA_IP=\"127.0.0.4\"export KUBE_API_HA_HOST=\"https://$&#123;KUBE_API_HA_IP&#125;\"## set eviction_headexport EVICTION_HARD=\"memory.available&lt;10%,nodefs.available&lt;10%\" 12export CURRENT_NODE=$&#123;NODE1&#125;export ETCD_NAME=\"etcd-01\" 12export CURRENT_NODE=$&#123;NODE2&#125;export ETCD_NAME=\"etcd-02\" 12export CURRENT_NODE=$&#123;NODE3&#125;export ETCD_NAME=\"etcd-03\" Generate etcd certificates12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970# Generate CA certs for etcdcat &gt;etcd-cs-ca-csr.json &lt;&lt;EOF&#123; \"CN\": \"MYZD etcd CS CA\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"Shanghai\", \"L\": \"Shanghai\", \"O\": \"MYZD\", \"OU\": \"System\" &#125; ], \"CA\": &#123; \"Expiry\": \"876000h\" &#125;&#125;EOFcfssl gencert -initca etcd-cs-ca-csr.json | cfssljson -bare etcd-cs-cacat &gt;etcd-peer-ca-csr.json &lt;&lt;EOF&#123; \"CN\": \"MYZD etcd peer CA\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"Shanghai\", \"L\": \"Shanghai\", \"O\": \"MYZD\", \"OU\": \"System\" &#125; ], \"CA\": &#123; \"Expiry\": \"876000h\" &#125;&#125;EOFcfssl gencert -initca etcd-peer-ca-csr.json | cfssljson -bare etcd-peer-ca# Generate peer certificateecho '&#123;\"CN\":\"etcd-peer\",\"hosts\":[\"\"],\"key\":&#123;\"algo\":\"rsa\",\"size\":2048&#125;&#125;' |\\cfssl gencert -ca=etcd-peer-ca.pem -ca-key=etcd-peer-ca-key.pem -config=ca-config.json \\-hostname=\"$&#123;MASTER_IPS&#125;,etcd-01,etcd-02,etcd-03\" - |\\cfssljson -bare etcd-peer# Generate server certificateecho '&#123;\"CN\":\"etcd-server\",\"hosts\":[\"\"],\"key\":&#123;\"algo\":\"rsa\",\"size\":2048&#125;&#125;' |\\cfssl gencert -ca=etcd-cs-ca.pem -ca-key=etcd-cs-ca-key.pem -config=ca-config.json \\-hostname=\"$&#123;MASTER_IPS&#125;,127.0.0.1,localhost,etcd-01,etcd-02,etcd-03\" - |\\cfssljson -bare etcd-server# Generate client certificatesecho '&#123;\"CN\":\"etcd-client\",\"hosts\":[\"\"],\"key\":&#123;\"algo\":\"rsa\",\"size\":2048&#125;&#125;' |\\cfssl gencert -ca=etcd-cs-ca.pem -ca-key=etcd-cs-ca-key.pem -config=ca-config.json - |\\cfssljson -bare etcd-client# Copy certs tp etcd config directory(/etc/etcd on each peer)mkdir etcdcp etcd-cs-ca.pem etcdcp etcd-peer-ca.pem etcdcp etcd-peer.pem etcdcp etcd-peer-key.pem etcdcp etcd-server.pem etcdcp etcd-server-key.pem etcdcp etcd-client.pem etcdcp etcd-client-key.pem etcd Generate kubernete certificates123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208# Generate CA certs for kebeletcat &gt;kubelet-ca-csr.json &lt;&lt;EOF&#123; \"CN\": \"MYZD kubelet CA\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"Shanghai\", \"L\": \"Shanghai\", \"O\": \"MYZD\", \"OU\": \"Kubernetes\" &#125; ], \"CA\": &#123; \"Expiry\": \"876000h\" &#125;&#125;EOFcfssl gencert -initca kubelet-ca-csr.json | cfssljson -bare kubelet-ca# Generate kubelet-client rsa key paircat &gt;kubelet-client-csr.json &lt;&lt;EOF&#123; \"CN\": \"kubelet-client\", \"hosts\": [], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"Shanghai\", \"L\": \"Shanghai\", \"O\": \"system:kebulet\", \"OU\": \"Kubernetes\" &#125; ]&#125;EOFcfssl gencert -ca=kubelet-ca.pem -ca-key=kubelet-ca-key.pem -config=ca-config.json kubelet-client-csr.json | cfssljson -bare kubelet-client-certificate# Generate CA certs for kubernetescat &gt;kubernetes-ca-csr.json &lt;&lt;EOF&#123; \"CN\": \"MYZD Kubernetes CA\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"Shanghai\", \"L\": \"Shanghai\", \"O\": \"MYZD\", \"OU\": \"System\" &#125; ], \"CA\": &#123; \"Expiry\": \"876000h\" &#125;&#125;EOFcfssl gencert -initca kubernetes-ca-csr.json | cfssljson -bare kube-ca# Generate service account rsa key pairopenssl genrsa -out kube-service-account.key 4096cat &gt;kube-admin-csr.json &lt;&lt;EOF&#123; \"CN\": \"admin\", \"hosts\": [], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"Shanghai\", \"L\": \"Shanghai\", \"O\": \"system:masters\", \"OU\": \"Kubernetes\" &#125; ]&#125;EOFcfssl gencert -ca=kube-ca.pem -ca-key=kube-ca-key.pem -config=ca-config.json \\kube-admin-csr.json | cfssljson -bare kube-admincat &gt;kube-controller-csr.json &lt;&lt;EOF&#123; \"CN\": \"system:kube-controller-manager\", \"hosts\": [], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"Shanghai\", \"L\": \"Shanghai\", \"O\": \"system\", \"OU\": \"Kubernetes\" &#125; ]&#125;EOFcfssl gencert -ca=kube-ca.pem -ca-key=kube-ca-key.pem -config=ca-config.json \\kube-controller-csr.json | cfssljson -bare kube-controllercat &gt;kube-scheduler-csr.json &lt;&lt;EOF&#123; \"CN\": \"system:kube-scheduler\", \"hosts\": [], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"Shanghai\", \"L\": \"Shanghai\", \"O\": \"system\", \"OU\": \"Kubernetes\" &#125; ]&#125;EOFcfssl gencert -ca=kube-ca.pem -ca-key=kube-ca-key.pem -config=ca-config.json \\kube-scheduler-csr.json | cfssljson -bare kube-schedulercat &gt;kube-proxy-csr.json &lt;&lt;EOF&#123; \"CN\": \"system:kube-proxy\", \"hosts\": [], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"Shanghai\", \"L\": \"Shanghai\", \"O\": \"system\", \"OU\": \"Kubernetes\" &#125; ]&#125;EOFcfssl gencert -ca=kube-ca.pem -ca-key=kube-ca-key.pem -config=ca-config.json \\kube-proxy-csr.json | cfssljson -bare kube-proxycat &gt;kubelet-csr.json &lt;&lt;EOF&#123; \"CN\": \"system:kubelet\", \"hosts\": [], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"Shanghai\", \"L\": \"Shanghai\", \"O\": \"system:node\", \"OU\": \"Kubernetes\" &#125; ]&#125;EOFcfssl gencert -ca=kube-ca.pem -ca-key=kube-ca-key.pem -config=ca-config.json \\kubelet-csr.json | cfssljson -bare kubelet# Generate apiserver https certsecho '&#123;\"CN\":\"kube-api\",\"hosts\":[\"\"],\"key\":&#123;\"algo\":\"rsa\",\"size\":2048&#125;&#125;' |\\cfssl gencert -ca=kube-ca.pem -ca-key=kube-ca-key.pem -config=ca-config.json \\-hostname=\"$&#123;MASTER_IPS&#125;,127.0.0.1,localhost,kube-api,$&#123;KUBE_API_CLUSTER_IP&#125;,$&#123;KUBE_API_HA_IP&#125;\" - |\\cfssljson -bare kube-api# Copy certs to kubernetes config directory(/etc/kubernetes on each master)mkdir kubernetescp kube-ca.pem kubernetescp kubelet.pem kubernetescp kubelet-key.pem kubernetescp kubelet-ca.pem kubernetes# masters onlycp etcd-cs-ca.pem kubernetescp etcd-client.pem kubernetescp etcd-client-key.pem kubernetescp kube-service-account.key kubernetescp kube-api.pem kubernetescp kube-api-key.pem kubernetescp kube-controller.pem kubernetescp kube-controller-key.pem kubernetescp kube-scheduler.pem kubernetescp kube-scheduler-key.pem kubernetescp kube-proxy.pem kubernetescp kube-proxy-key.pem kubernetescp kubelet-client-certificate.pem kubernetescp kubelet-client-certificate-key.pem kubernetesmkdir bakcp *.pem bak Generate kubernete configurations12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788# Generate config for kubernetes components.cat &gt;kubernetes/kubecfg-controller.yml &lt;&lt;EOFapiVersion: v1kind: Configclusters:- cluster: api-version: v1 certificate-authority: /etc/kubernetes/kube-ca.pem server: \"$&#123;KUBE_API_HA_HOST&#125;\" name: \"local\"contexts:- context: cluster: \"local\" user: \"kube-controller\" name: \"Default\"current-context: \"Default\"users:- name: \"kube-controller\" user: client-certificate: /etc/kubernetes/kube-controller.pem client-key: /etc/kubernetes/kube-controller-key.pemEOFcat &gt;kubernetes/kubecfg-scheduler.yml &lt;&lt;EOFapiVersion: v1kind: Configclusters:- cluster: api-version: v1 certificate-authority: /etc/kubernetes/kube-ca.pem server: \"$&#123;KUBE_API_HA_HOST&#125;\" name: \"local\"contexts:- context: cluster: \"local\" user: \"kube-scheduler\" name: \"Default\"current-context: \"Default\"users:- name: \"kube-scheduler\" user: client-certificate: /etc/kubernetes/kube-scheduler.pem client-key: /etc/kubernetes/kube-scheduler-key.pemEOFcat &gt;kubernetes/kubecfg-proxy.yml &lt;&lt;EOFapiVersion: v1kind: Configclusters:- cluster: api-version: v1 certificate-authority: /etc/kubernetes/kube-ca.pem server: \"$&#123;KUBE_API_HA_HOST&#125;\" name: \"local\"contexts:- context: cluster: \"local\" user: \"kube-proxy\" name: \"Default\"current-context: \"Default\"users:- name: \"kube-proxy\" user: client-certificate: /etc/kubernetes/kube-proxy.pem client-key: /etc/kubernetes/kube-proxy-key.pemEOFcat &gt;kubernetes/kubecfg-kubelet.yml &lt;&lt;EOFapiVersion: v1kind: Configclusters:- cluster: api-version: v1 certificate-authority: /etc/kubernetes/kube-ca.pem server: \"$&#123;KUBE_API_HA_HOST&#125;\" name: \"local\"contexts:- context: cluster: \"local\" user: \"kubelet\" name: \"Default\"current-context: \"Default\"users:- name: \"kubelet\" user: client-certificate: /etc/kubernetes/kubelet.pem client-key: /etc/kubernetes/kubelet-key.pemEOF Run basic services on each node123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157# daemons with docker-compose filesmkdir daemonscat &gt;daemons/etcd.yml &lt;&lt;EOFversion: '2'services: etcd: image: bestmike007/etcd:v3.2.13 container_name: etcd restart: always network_mode: host volumes: - /srv/etcd:/$&#123;ETCD_NAME&#125;.etcd - /etc/etcd:/certs:ro environment: ETCD_NAME: $&#123;ETCD_NAME&#125; ETCD_INITIAL_ADVERTISE_PEER_URLS: https://$&#123;CURRENT_NODE&#125;:2380 ETCD_LISTEN_PEER_URLS: https://$&#123;CURRENT_NODE&#125;:2380 ETCD_ADVERTISE_CLIENT_URLS: https://$&#123;CURRENT_NODE&#125;:2379 ETCD_LISTEN_CLIENT_URLS: https://$&#123;CURRENT_NODE&#125;:2379,https://127.0.0.1:2379 ETCD_INITIAL_CLUSTER_TOKEN: $&#123;ETCD_INITIAL_CLUSTER_TOKEN&#125; ETCD_INITIAL_CLUSTER: $&#123;ETCD_INITIAL_CLUSTER&#125; ETCD_CLIENT_CERT_AUTH: \"true\" ETCD_TRUSTED_CA_FILE: /certs/etcd-cs-ca.pem ETCD_CERT_FILE: /certs/etcd-server.pem ETCD_KEY_FILE: /certs/etcd-server-key.pem ETCD_PEER_CLIENT_CERT_AUTH: \"true\" ETCD_PEER_TRUSTED_CA_FILE: /certs/etcd-peer-ca.pem ETCD_PEER_CERT_FILE: /certs/etcd-peer.pem ETCD_PEER_KEY_FILE: /certs/etcd-peer-key.pemEOFcat &gt;daemons/kube-master.yml &lt;&lt;EOFversion: '2'services: kube-api: image: bestmike007/hyperkube:v1.9.1 container_name: kube-api restart: always network_mode: host volumes: - /etc/kubernetes:/etc/kubernetes:ro command: \"/usr/local/bin/kube-apiserver \\ --apiserver-count=3 \\ --allow_privileged=true \\ --service-cluster-ip-range=$&#123;SERVICE_CLUSTER_CIDR&#125; \\ --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname \\ --admission-control=ServiceAccount,NamespaceLifecycle,LimitRanger,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds \\ --runtime-config=batch/v2alpha1 \\ --runtime-config=authentication.k8s.io/v1beta1=true \\ --runtime-config=extensions/v1beta1/podsecuritypolicy=true \\ --storage-backend=etcd3 \\ --etcd-servers=$&#123;ETCD_SERVERS&#125; \\ --etcd-cafile=/etc/kubernetes/etcd-cs-ca.pem \\ --etcd-certfile=/etc/kubernetes/etcd-client.pem \\ --etcd-keyfile=/etc/kubernetes/etcd-client-key.pem \\ --client-ca-file=/etc/kubernetes/kube-ca.pem \\ --service-account-key-file=/etc/kubernetes/kube-service-account.key \\ --tls-ca-file=/etc/kubernetes/kube-ca.pem \\ --tls-cert-file=/etc/kubernetes/kube-api.pem \\ --tls-private-key-file=/etc/kubernetes/kube-api-key.pem \\ --authorization-mode=RBAC \\ --kubelet-client-certificate=/etc/kubernetes/kubelet-client-certificate.pem \\ --kubelet-client-key=/etc/kubernetes/kubelet-client-certificate-key.pem \\ --v=4\" kube-controller: image: bestmike007/hyperkube:v1.9.1 container_name: kube-controller restart: always network_mode: host volumes: - /etc/kubernetes:/etc/kubernetes:ro command: \"/usr/local/bin/kube-controller-manager \\ --address=0.0.0.0 \\ --leader-elect=true \\ --kubeconfig=/etc/kubernetes/kubecfg-controller.yml \\ --enable-hostpath-provisioner=false \\ --node-monitor-grace-period=40s \\ --pod-eviction-timeout=5m0s \\ --v=2 \\ --allocate-node-cidrs=true \\ --cluster-cidr=$&#123;CLUSTER_CIDR&#125; \\ --service-cluster-ip-range=$&#123;SERVICE_CLUSTER_CIDR&#125; \\ --service-account-private-key-file=/etc/kubernetes/kube-service-account.key \\ --root-ca-file=/etc/kubernetes/kube-ca.pem \\ --use-service-account-credentials=true\" kube-scheduler: image: bestmike007/hyperkube:v1.9.1 container_name: kube-scheduler restart: always network_mode: host volumes: - /etc/kubernetes:/etc/kubernetes:ro command: \"/usr/local/bin/kube-scheduler \\ --leader-elect=true \\ --v=2 \\ --kubeconfig=/etc/kubernetes/kubecfg-scheduler.yml \\ --address=0.0.0.0\"EOFcat &gt;daemons/kube-node.yml &lt;&lt;EOFversion: '2'services: kubelet: image: bestmike007/hyperkube:v1.9.1 container_name: kubelet restart: always privileged: true pid: host network_mode: host volumes: - /var/log:/var/log - /dev:/dev - /run:/run - /sys:/sys:ro - /sys/fs/cgroup:/sys/fs/cgroup:rw - /var/run:/var/run:rw - /var/lib/docker/:/var/lib/docker:rw - /var/lib/kubelet/:/var/lib/kubelet:shared - /etc/kubernetes:/etc/kubernetes:ro - /etc/cni:/etc/cni:ro - /opt/cni/bin:/opt/cni/local/bin:rw command: bash -c \"cp /opt/cni/bin/* /opt/cni/local/bin &amp;&amp; \\ /usr/local/bin/kubelet \\ --v=2 \\ --address=0.0.0.0 \\ --anonymous-auth=false \\ --client-ca-file=/etc/kubernetes/kubelet-ca.pem \\ --cluster-domain=cluster.local \\ --pod-infra-container-image=bestmike007/pause-amd64:3.1 \\ --cgroups-per-qos=True \\ --enforce-node-allocatable= \\ --hostname-override=$&#123;CURRENT_NODE&#125; \\ --cluster-dns=$&#123;CLUSTER_DNS&#125; \\ --network-plugin=cni \\ --cni-conf-dir=/etc/cni/net.d \\ --cni-bin-dir=/opt/cni/local/bin \\ --resolv-conf=/etc/resolv.conf \\ --allow-privileged=true \\ --cloud-provider= \\ --kubeconfig=/etc/kubernetes/kubecfg-kubelet.yml \\ --require-kubeconfig=True \\ --fail-swap-on=false \\ --eviction-hard='$&#123;EVICTION_HARD&#125;'\" kube-proxy: image: bestmike007/hyperkube:v1.9.1 container_name: kube-proxy restart: always privileged: true network_mode: host volumes: - /etc/kubernetes:/etc/kubernetes:ro command: \"/usr/local/bin/kube-proxy \\ --healthz-bind-address=0.0.0.0 \\ --kubeconfig=/etc/kubernetes/kubecfg-proxy.yml \\ --v=2\"EOF 123docker-compose -f ~/daemons/etcd.yml up -ddocker-compose -f ~/daemons/kube-master.yml up -ddocker-compose -f ~/daemons/kube-node.yml up -d Post-config cluster12345678910# check etcd cluster health# etcdctl --ca-file=/certs/etcd-cs-ca.pem --endpoints=https://127.0.0.1:2379 --key-file=/certs/etcd-client-key.pem --cert-file=/certs/etcd-client.pem cluster-health# use the first master, run the following command after `docker exec -it kubelet sh`kubectl create clusterrolebinding \\kubelet-node-binding --clusterrole=system:node --user=system:kubelet# set up weave cni network.# 注意: 这里的IPALLOC_RANGE与SERVICE_CLUSTER_CIDR相同kubectl apply -f \\\"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')&amp;env.IPALLOC_RANGE=172.18.64.0/18\" Firewall12345678910111213141516# kube-apiufw allow 6443/tcp# etcdufw allow 2379/tcpufw allow 2380/tcp# weaveufw allow proto any from $&#123;NODE1&#125; to any port 6783ufw allow proto any from $&#123;NODE2&#125; to any port 6783ufw allow proto any from $&#123;NODE3&#125; to any port 6783ufw allow proto udp from $&#123;NODE1&#125; to any port 6784ufw allow proto udp from $&#123;NODE2&#125; to any port 6784ufw allow proto udp from $&#123;NODE3&#125; to any port 6784# kubeletufw allow proto tcp from $&#123;NODE1&#125; to any port 10250ufw allow proto tcp from $&#123;NODE2&#125; to any port 10250ufw allow proto tcp from $&#123;NODE3&#125; to any port 10250 Tear down1234567docker-compose -f daemons/kube-master.yml downdocker-compose -f daemons/kube-node.yml downdocker-compose -f daemons/etcd.yml downdocker rm $(docker ps -aq) -frebootrm -rf /var/lib/kubelet /srv/etcd /etc/cni /etc/kubernetes /etc/etcd","tags":[{"name":"笔记","slug":"笔记","permalink":"http://localhost:4000/tags/笔记/"}]},{"title":"部署高可用的k8s集群","date":"2018-01-25T05:58:57.000Z","path":"2018/01/25/kubernetes-ha/","text":"Kubernetes HA 摘自 在CentOS上部署kubernetes1.6集群 创建TLS证书和密钥安装CFSSL(源码包安装)12345678910111213wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64chmod +x cfssl_linux-amd64mv cfssl_linux-amd64 /usr/local/bin/cfsslwget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64chmod +x cfssljson_linux-amd64mv cfssljson_linux-amd64 /usr/local/bin/cfssljsonwget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64chmod +x cfssl-certinfo_linux-amd64mv cfssl-certinfo_linux-amd64 /usr/local/bin/cfssl-certinfoexport PATH=/usr/local/bin:$PATH 创建CACA配置文件1234567891011121314151617181920212223242526mkdir /root/sslcd /root/sslcfssl print-defaults config &gt; config.jsoncfssl print-defaults csr &gt; csr.json# 根据config.json文件的格式创建如下的ca-config.json文件# 过期时间设置成了 87600hcat &gt; ca-config.json &lt;&lt;EOF&#123; \"signing\": &#123; \"default\": &#123; \"expiry\": \"87600h\" &#125;, \"profiles\": &#123; \"kubernetes\": &#123; \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"87600h\" &#125; &#125; &#125;&#125;EOF ca-config.json：可以定义多个 profiles，分别指定不同的过期时间、使用场景等参数；后续在签名证书时使用某个 profile； signing：表示该证书可用于签名其它证书；生成的 ca.pem 证书中 CA=TRUE； server auth：表示client可以用该 CA 对server提供的证书进行验证； client auth：表示server可以用该CA对client提供的证书进行验证； 创建CA证书签名请求123456789101112131415161718cat &gt; ca-csr.json &lt;&lt;EOF&#123; \"CN\": \"kubernetes\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" &#125; ]&#125;EOF “CN”：Common Name，kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)；浏览器使用该字段验证网站是否合法； “O”：Organization，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group)； 生成 CA 证书和私钥123$ cfssl gencert -initca ca-csr.json | cfssljson -bare ca$ ls ca*ca-config.json ca.csr ca-csr.json ca-key.pem ca.pem 创建kubernetes证书创建 kubernetes 证书签名请求文件12345678910111213141516171819202122232425262728293031cat kubernetes-csr.json &lt;&lt;EOF&#123; \"CN\": \"kubernetes\", \"hosts\": [ \"127.0.0.1\", \"172.20.0.112\", \"172.20.0.113\", \"172.20.0.114\", \"172.20.0.115\", \"10.254.0.1\", \"kubernetes\", \"kubernetes.default\", \"kubernetes.default.svc\", \"kubernetes.default.svc.cluster\", \"kubernetes.default.svc.cluster.local\" ], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" &#125; ]&#125;EOF 如果 hosts 字段不为空则需要指定授权使用该证书的 IP 或域名列表，由于该证书后续被 etcd 集群和 kubernetes master 集群使用，所以上面分别指定了 etcd 集群、kubernetes master 集群的主机 IP 和 kubernetes 服务的服务 IP（一般是 kube-apiserver 指定的 service-cluster-ip-range 网段的第一个IP，如 10.254.0.1。 hosts 中的内容可以为空，即使按照上面的配置，向集群中增加新节点后也不需要重新生成证书。 生成 kubernetes 证书和私钥123$ cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes$ ls kubernetes*kubernetes.csr kubernetes-csr.json kubernetes-key.pem kubernetes.pem 创建admin证书创建 admin 证书签名请求文件12345678910111213141516171819cat admin-csr.json &lt;&lt;EOF&#123; \"CN\": \"admin\", \"hosts\": [], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"system:masters\", \"OU\": \"System\" &#125; ]&#125;EOF 后续 kube-apiserver 使用 RBAC 对客户端(如 kubelet、kube-proxy、Pod)请求进行授权； kube-apiserver 预定义了一些 RBAC 使用的 RoleBindings，如 cluster-admin 将 Group system:masters 与 Role cluster-admin 绑定，该 Role 授予了调用kube-apiserver 的所有 API的权限； OU 指定该证书的 Group 为 system:masters，kubelet 使用该证书访问 kube-apiserver 时 ，由于证书被 CA 签名，所以认证通过，同时由于证书用户组为经过预授权的 system:masters，所以被授予访问所有 API 的权限； 生成 admin 证书和私钥123$ cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin$ ls admin*admin.csr admin-csr.json admin-key.pem admin.pem 创建 kube-proxy 证书创建kube-proxy证书签名请求文件12345678910111213141516171819cat kube-proxy-csr.json &lt;&lt;EOF&#123; \"CN\": \"system:kube-proxy\", \"hosts\": [], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" &#125; ]&#125;EOF 生成 kube-proxy 客户端证书和私钥123$ cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy$ ls kube-proxy*kube-proxy.csr kube-proxy-csr.json kube-proxy-key.pem kube-proxy.pem 校验证书opsnssl1openssl x509 -noout -text -in kubernetes.pem cfssl-certinfo1cfssl-certinfo -cert kubernetes.pem 分发证书12mkdir -p /etc/kubernetes/sslcp *.pem /etc/kubernetes/ssl 安装kubectl命令行工具下载 kubectl1234wget https://dl.k8s.io/v1.8.4/kubernetes-client-linux-amd64.tar.gztar -xzvf kubernetes-client-linux-amd64.tar.gzcp kubernetes/client/bin/kube* /usr/bin/chmod a+x /usr/bin/kube* 创建kubeconfig文件创建TLS Bootsrapping Token1234export BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d ' ')cat &gt; /etc/kubernetes/token.csv &lt;&lt;EOF$&#123;BOOTSTRAP_TOKEN&#125;,kubelet-bootstrap,10001,\"system:kubelet-bootstrap\"EOF 创建 kubelet bootstrapping kubeconfig 文件12345678910111213141516171819202122cd /etc/kubernetesexport KUBE_APISERVER=\"https://192.168.10.56:6443\"# 设置集群参数kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=$&#123;KUBE_APISERVER&#125; \\ --kubeconfig=bootstrap.kubeconfig# 设置客户端认证参数kubectl config set-credentials kubelet-bootstrap \\ --token=$&#123;BOOTSTRAP_TOKEN&#125; \\ --kubeconfig=bootstrap.kubeconfig# 设置上下文参数kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=bootstrap.kubeconfig# 设置默认上下文kubectl config use-context default --kubeconfig=bootstrap.kubeconfig --embed-certs 为 true 时表示将 certificate-authority 证书写入到生成的 bootstrap.kubeconfig 文件中. 设置客户端认证参数时没有指定秘钥和证书, 后续由 kube-apiserver 自动生成. 创建 kube-proxy kubeconfig 文件12345678910111213141516171819# 设置集群参数kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=$&#123;KUBE_APISERVER&#125; \\ --kubeconfig=kube-proxy.kubeconfig# 设置客户端认证参数kubectl config set-credentials kube-proxy \\ --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \\ --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig# 设置上下文参数kubectl config set-context default \\ --cluster=kubernetes \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig# 设置默认上下文kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig 若上述操作不是在/etc/kubernetes中执行, 则需要将生成文件cp到/etc/kubernetes中. Etcd集群下载并安装etcdTLS证书沿用kubernetes证书, 如有需要, 可按创建TLS证书和密钥 123wget https://github.com/coreos/etcd/releases/download/v3.2.10/etcd-v3.2.10-linux-amd64.tar.gztar -xvf etcd-v3.2.10-linux-amd64.tar.gzmv etcd-v3.2.10-linux-amd64/etcd* /usr/local/bin 或者直接拷贝其他主机的证书 1scp root@192.168.10.56:/etc/kubernetes/ssl/* /etc/kubernetes/ssl/ 创建 etcd 的systamd unit 文件/etc/systemd/system/etcd.service, 注意替换IP地址为你自己的etcd集群的主机IP. 123456789101112131415161718192021222324252627282930313233[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.targetDocumentation=https://github.com/coreos[Service]Type=notifyWorkingDirectory=/var/lib/etcd/EnvironmentFile=-/etc/etcd/etcd.confExecStart=/usr/local/bin/etcd \\ --name $&#123;ETCD_NAME&#125; \\ --cert-file=/etc/kubernetes/ssl/kubernetes.pem \\ --key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/kubernetes.pem \\ --peer-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls $&#123;ETCD_INITIAL_ADVERTISE_PEER_URLS&#125; \\ --listen-peer-urls $&#123;ETCD_LISTEN_PEER_URLS&#125; \\ --listen-client-urls $&#123;ETCD_LISTEN_CLIENT_URLS&#125;,https://127.0.0.1:2379 \\ --advertise-client-urls $&#123;ETCD_ADVERTISE_CLIENT_URLS&#125; \\ --initial-cluster-token $&#123;ETCD_INITIAL_CLUSTER_TOKEN&#125; \\ --initial-cluster infra1=https://192.168.10.56:2380,infra2=https://192.168.10.57:2380,infra3=https://192.168.10.58:2380 \\ --initial-cluster-state new \\ --data-dir=$&#123;ETCD_DATA_DIR&#125;Restart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.target 指定 etcd 的工作目录为 /var/lib/etcd,数据目录为 /var/lib/etcd,需在启动服务前创建这两个目录. 环境变量配置文件/etc/etcd/etcd.conf 123456789101112cat &gt; /etc/etcd/etcd.conf &lt;&lt;EOF# [member]ETCD_NAME=infra1ETCD_DATA_DIR=\"/var/lib/etcd\"ETCD_LISTEN_PEER_URLS=\"https://192.168.10.57:2380\"ETCD_LISTEN_CLIENT_URLS=\"https://192.168.10.57:2379\"#[cluster]ETCD_INITIAL_ADVERTISE_PEER_URLS=\"https://192.168.10.57:2380\"ETCD_INITIAL_CLUSTER_TOKEN=\"etcd-cluster\"ETCD_ADVERTISE_CLIENT_URLS=\"https://192.168.10.57:2379\"EOF 在另外的主机上添加同样的文件, 对应的IP改为主机IP, ETCD_NAME也相应修改. 启动 etcd 服务启动之前, 需要配置防火墙, 否则可能会出现connect refused问题 123ufw allow from 192.168.10.56ufw allow from 192.168.10.57ufw allow from 192.168.10.58 启动etcd服务 1234systemctl daemon-reloadsystemctl enable etcdsystemctl start etcdsystemctl status etcd 在其他主机上执行上述安装, 配置操作, 启动所有主机上的etcd服务. 验证服务12345etcdctl \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --cert-file=/etc/kubernetes/ssl/kubernetes.pem \\ --key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\ cluster-health 如果直接执行上述命令出现如下错误: 123cluster may be unhealthy: failed to list membersError: client: etcd cluster is unavailable or misconfigured; error #0: malformed HTTP response &quot;\\x15\\x03\\x01\\x00\\x02\\x02&quot;; error #1: dial tcp 127.0.0.1:4001: getsockopt: connection refused 执行export ETCDCTL_ENDPOINT=https://127.0.0.1:2379修改ETCDCTL_ENDPOINT环境变量, 之后再次执行, 则可以看到结果. 1234member 8f573ae51eacb66 is healthy: got healthy result from https://192.168.10.57:2379member 42c120089619ad70 is healthy: got healthy result from https://192.168.10.58:2379member 6bc3e880c7cf3ed7 is healthy: got healthy result from https://192.168.10.56:2379cluster is healthy 结果最后一行为 cluster is healthy 时表示集群服务正常. 若是出现member配置错误, 修改配置重新执行前, 需要删除/var/lib/etcd/member文件夹 PS: 若有个坑货在你部署的时候乱弄你的服务器, 可能会导致etcd member健康检查成功, 但主机上的etcd也创建成功, 但是连接不上集群. 暂时只想到重新启动所有etcd服务的解决方法: 123456789# 停止并删除工作目录的member文件夹systemctl stop etcd.servicesystemctl disable etcd.servicerm -r /var/lib/etcd/member/# 重新启动etcd集群systemctl daemon-reloadsystemctl enable etcdsystemctl start etcdsystemctl status etcd 部署master集群安装kubernetes 123wget https://dl.k8s.io/v1.8.4/kubernetes-server-linux-amd64.tar.gztar -xzvf kubernetes-server-linux-amd64.tar.gzcp -r kubernetes/server/bin/&#123;kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet&#125; /usr/local/bin/ 关闭swap 1swapoff -a docker代理, 所以需要设置docker的代理 1234567mkdir -p /etc/systemd/system/docker.service.dcat &gt; /etc/systemd/system/docker.service.d/http-proxy.conf &lt;&lt; EOF[Service]Environment=\"HTTP_PROXY=http://192.168.32.10:6780\"Environment=\"HTTPS_PROXY=http://192.168.32.10:6780\"Environment=\"NO_PROXY=.aliyun.com,.aliyuncs.com,.daocloud.io,.cn,localhost\"EOF note: no_proxy不支持通配符 部署master节点证书上述已经创建好, 可直接使用 下载最新版二进制文件从 CHANGELOG页面 下载 client 或 server tarball 文件 123456wget https://dl.k8s.io/v1.8.4/kubernetes-server-linux-amd64.tar.gztar -xzvf kubernetes-server-linux-amd64.tar.gzcd kubernetestar -xzvf kubernetes-src.tar.gz#将二进制文件拷贝到指定路径cp -r server/bin/&#123;kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet&#125; /usr/local/bin/ 配置和启动kube-apiserver创建kube-apiserver的service配置文件/etc/systemd/system/kube-apiserver.service 1234567891011121314151617181920212223242526[Unit]Description=Kubernetes API ServiceDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.targetAfter=etcd.service[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/apiserverExecStart=/usr/local/bin/kube-apiserver \\ $KUBE_LOGTOSTDERR \\ $KUBE_LOG_LEVEL \\ $KUBE_ETCD_SERVERS \\ $KUBE_API_ADDRESS \\ $KUBE_API_PORT \\ $KUBELET_PORT \\ $KUBE_ALLOW_PRIV \\ $KUBE_SERVICE_ADDRESSES \\ $KUBE_ADMISSION_CONTROL \\ $KUBE_API_ARGSRestart=on-failureType=notifyLimitNOFILE=65536[Install]WantedBy=multi-user.target /etc/kubernetes/config: 12345678910111213141516171819202122232425cat &gt; /etc/kubernetes/config &lt;&lt;EOF#### kubernetes system config## The following values are used to configure various aspects of all# kubernetes services, including## kube-apiserver.service# kube-controller-manager.service# kube-scheduler.service# kubelet.service# kube-proxy.service# logging to stderr means we get it in the systemd journalKUBE_LOGTOSTDERR=\"--logtostderr=true\"# journal message level, 0 is debugKUBE_LOG_LEVEL=\"--v=0\"# Should this cluster be allowed to run privileged docker containersKUBE_ALLOW_PRIV=\"--allow-privileged=true\"# How the controller-manager, scheduler, and proxy find the apiserver#KUBE_MASTER=\"--master=http://sz-pg-oam-docker-test-001.tendcloud.com:8080\"KUBE_MASTER=\"--master=http://192.168.10.58:8080\"EOF 该配置文件同时被kube-apiserver, kube-controller-manager, kube-scheduler, kubelet, kube-proxy使用. apiserver配置文件/etc/kubernetes/apiserver内容为: 1234567891011121314151617181920212223242526272829cat &gt; /etc/kubernetes/apiserver &lt;&lt;EOF##### kubernetes system config#### The following values are used to configure the kube-apiserver##### The address on the local server to listen to.#KUBE_API_ADDRESS=\"--insecure-bind-address=sz-pg-oam-docker-test-001.tendcloud.com\"KUBE_API_ADDRESS=\"--advertise-address=192.168.10.58 --bind-address=192.168.10.58 --insecure-bind-address=192.168.10.58\"### The port on the local server to listen on.#KUBE_API_PORT=\"--port=8080\"### Port minions listen on#KUBELET_PORT=\"--kubelet-port=10250\"### Comma separated list of nodes in the etcd clusterKUBE_ETCD_SERVERS=\"--etcd-servers=https://192.168.10.56:2379,https://192.168.10.57:2379,https://192.168.10.58:2379\"### Address range to use for servicesKUBE_SERVICE_ADDRESSES=\"--service-cluster-ip-range=10.254.0.0/16\"### default admission control policiesKUBE_ADMISSION_CONTROL=\"--admission-control=ServiceAccount,NamespaceLifecycle,NamespaceExists,LimitRanger,ResourceQuota\"### Add your own!KUBE_API_ARGS=\"--authorization-mode=RBAC --runtime-config=rbac.authorization.k8s.io/v1beta1 --kubelet-https=true --experimental-bootstrap-token-auth --token-auth-file=/etc/kubernetes/token.csv --service-node-port-range=30000-32767 --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem --client-ca-file=/etc/kubernetes/ssl/ca.pem --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem --etcd-cafile=/etc/kubernetes/ssl/ca.pem --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem --enable-swagger-ui=true --apiserver-count=3 --audit-log-maxage=30 --audit-log-maxbackup=3 --audit-log-maxsize=100 --audit-log-path=/var/lib/audit.log --event-ttl=1h\"EOF --admission-control 值必须包含 ServiceAccount --bind-address 不能为 127.0.0.1 启动kube-apiserver1234systemctl daemon-reloadsystemctl enable kube-apiserversystemctl start kube-apiserversystemctl status kube-apiserver 配置和启动 kube-controller-manager创建 kube-controller-manager的serivce配置文件service配置文件/etc/systemd/system/kube-controller-manager.service 1234567891011121314151617[Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/controller-managerExecStart=/usr/local/bin/kube-controller-manager \\ $KUBE_LOGTOSTDERR \\ $KUBE_LOG_LEVEL \\ $KUBE_MASTER \\ $KUBE_CONTROLLER_MANAGER_ARGSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target 配置文件/etc/kubernetes/controller-manager 123456789cat &gt; /etc/kubernetes/controller-manager &lt;&lt;EOF#### The following values are used to configure the kubernetes controller-manager# defaults from config and apiserver should be adequate# Add your own!KUBE_CONTROLLER_MANAGER_ARGS=\"--address=127.0.0.1 --service-cluster-ip-range=10.254.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem --root-ca-file=/etc/kubernetes/ssl/ca.pem --leader-elect=true\"EOF 启动 kube-controller-manager123systemctl daemon-reloadsystemctl enable kube-controller-managersystemctl start kube-controller-manager 配置和启动 kube-scheduler创建 kube-scheduler的serivce配置文件serivce文件/etc/systemd/system/kube-scheduler.service 1234567891011121314151617[Unit]Description=Kubernetes Scheduler PluginDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/schedulerExecStart=/usr/local/bin/kube-scheduler \\ $KUBE_LOGTOSTDERR \\ $KUBE_LOG_LEVEL \\ $KUBE_MASTER \\ $KUBE_SCHEDULER_ARGSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target 配置文件/etc/kubernetes/scheduler 123456789cat &gt; /etc/kubernetes/scheduler &lt;&lt;EOF#### kubernetes scheduler config# default config should be adequate# Add your own!KUBE_SCHEDULER_ARGS=\"--leader-elect=true --address=127.0.0.1\"EOF --address 值必须为 127.0.0.1，因为当前 kube-apiserver 期望 scheduler 和 controller-manager 在同一台机器 启动 kube-scheduler123systemctl daemon-reloadsystemctl enable kube-schedulersystemctl start kube-scheduler 验证 master 节点功能1234567$ kubectl get componentstatusesNAME STATUS MESSAGE ERRORscheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125; etcd-1 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125; etcd-2 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125; 若出现The connection to the server localhost:8080 was refused - did you specify the right host or port?错误, 请运行kubectl --server=192.168.10.56:8080 get componentstatuses, 自行修改为主机IP. apiserver不要指定–insecure-bind-address, 则不会出现上述问题. 之后再另外两台主机上启动apiserver, controller-maneger, scheduler. 记得cp /etc/kubernetes/token.csv 部署node节点基础配置检查前几部生成的ca文件, 及config文件是否都有. 安装网络插件Flanneld(可跳过)下载二进制文件 123wget https://github.com/coreos/flannel/releases/download/v0.9.1/flannel-v0.9.1-linux-amd64.tar.gztar -xzvf flannel-v0.9.1-linux-amd64.tar.gzcp flanneld /usr/bin/ /etc/systemd/system/flanneld.service 12345678910111213141516171819202122[Unit]Description=Flanneld overlay address etcd agentAfter=network.targetAfter=network-online.targetWants=network-online.targetAfter=etcd.serviceBefore=docker.service[Service]Type=notifyEnvironmentFile=/etc/sysconfig/flanneldEnvironmentFile=-/etc/sysconfig/docker-networkExecStart=/usr/bin/flanneld \\ -etcd-endpoints=$&#123;ETCD_ENDPOINTS&#125; \\ -etcd-prefix=$&#123;ETCD_PREFIX&#125; \\ $FLANNEL_OPTIONSExecStartPost=/usr/libexec/flannel/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/dockerRestart=on-failure[Install]WantedBy=multi-user.targetRequiredBy=docker.service /etc/sysconfig/flanneld配置文件 1234567891011121314mkdir /etc/sysconfigcat &gt; /etc/sysconfig/flanneld &lt;&lt;EOF# Flanneld configuration options # etcd url location. Point this to the server where etcd runsETCD_ENDPOINTS=&quot;https://192.168.10.56:2379,https://192.168.10.57:2379,https://192.168.10.58:2379&quot;# etcd config key. This is the configuration key that flannel queries# For address range assignmentETCD_PREFIX=&quot;/kube-centos/network&quot;# Any additional options that you want to passFLANNEL_OPTIONS=&quot;-etcd-cafile=/etc/kubernetes/ssl/ca.pem -etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem -etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem&quot;EOF 在etcd中创建网络配置12345678910etcdctl --endpoints=https://192.168.10.56:2379,https://192.168.10.57:2379,https://192.168.10.558:2379 \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --cert-file=/etc/kubernetes/ssl/kubernetes.pem \\ --key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\ mkdir /kube-centos/networketcdctl --endpoints=https://192.168.10.56:2379,https://192.168.10.57:2379,https://192.168.10.558:2379 \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --cert-file=/etc/kubernetes/ssl/kubernetes.pem \\ --key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\ mk /kube-centos/network/config '&#123;\"Network\":\"172.30.0.0/16\",\"SubnetLen\":24,\"Backend\":&#123;\"Type\":\"vxlan\"&#125;&#125;' 配置Docker如果你不是使用yum安装的flanneld, 需要执行下载文件解压出的mk-docker-opts.sh文件. 这个文件是用来Generate Docker daemon options based on flannel env file. 执行./mk-docker-opts.sh -i将会生成如下两个文件环境变量文件 重启docker 1234567891011service docker stopdockerd --bip=$&#123;FLANNEL_SUBNET&#125; --mtu=$&#123;FLANNEL_MTU&#125;# 设置docker0网桥的IP地址ifconfig docker0 $FLANNEL_SUBNET# 环境变量配置cat &gt; /etc/systemd/system/docker.service.d/flannel.conf &lt;&lt;EOF[Service]EnvironmentFile=-/run/flannel/dockerEnvironmentFile=-/run/docker_opts.envEnvironmentFile=-/run/flannel/subnet.envEOF 安装和配置 kubelet下载最新的 kubelet 和 kube-proxy 二进制文件12345wget https://dl.k8s.io/v1.8.4/kubernetes-server-linux-amd64.tar.gztar -xzvf kubernetes-server-linux-amd64.tar.gzcd kubernetestar -xzvf kubernetes-src.tar.gzcp -r ./server/bin/&#123;kube-proxy,kubelet&#125; /usr/local/bin/ 创建 kubelet 的service配置文件/etc/systemd/system/kubelet.service 123456789101112131415161718192021222324[Unit]Description=Kubernetes Kubelet ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=docker.serviceRequires=docker.service[Service]WorkingDirectory=/var/lib/kubeletEnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/kubeletExecStart=/usr/local/bin/kubelet \\ $KUBE_LOGTOSTDERR \\ $KUBE_LOG_LEVEL \\ $KUBELET_API_SERVER \\ $KUBELET_ADDRESS \\ $KUBELET_PORT \\ $KUBELET_HOSTNAME \\ $KUBE_ALLOW_PRIV \\ $KUBELET_POD_INFRA_CONTAINER \\ $KUBELET_ARGSRestart=on-failure[Install]WantedBy=multi-user.target kubelet的配置文件/etc/kubernetes/kubelet 1234567891011121314151617181920212223cat &gt; /etc/kubernetes/kubelet &lt;&lt;EOF##### kubernetes kubelet (minion) config### The address for the info server to serve on (set to 0.0.0.0 or \"\" for all interfaces)KUBELET_ADDRESS=\"--address=192.168.10.58\"### The port for the info server to serve on#KUBELET_PORT=\"--port=10250\"### You may leave this blank to use the actual hostnameKUBELET_HOSTNAME=\"--hostname-override=192.168.10.58\"### location of the api-server已过时# KUBELET_API_SERVER=\"--api-servers=http://192.168.10.58:8080\"### pod infrastructure containerKUBELET_POD_INFRA_CONTAINER=\"--pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest\"### Add your own!# --cgroup-driver=systemd 的配置需要跟docker一样KUBELET_ARGS=\"--cgroup-driver=cgroupfs --cluster-dns=10.254.0.2 --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig --kubeconfig=/etc/kubernetes/kubelet.kubeconfig --require-kubeconfig --cert-dir=/etc/kubernetes/ssl --cluster-domain=cluster.local --hairpin-mode promiscuous-bridge --serialize-image-pulls=false\"EOF handbook上说, /etc/kubernetes/kubelet.kubeconfig可暂时不创建, 在之后的步骤中会自动创建, 我是直接cp ~/.kube/config /etc/kubernetes/kulelet.kubeconfg, config文件是否可以不存在, 之后需要再验证 启动kubelet1234systemctl daemon-reloadsystemctl enable kubeletsystemctl start kubeletsystemctl status kubelet 配置 kube-proxy创建 kube-proxy 的service配置文件/etc/systemd/system/kube-proxy.service 123456789101112131415161718[Unit]Description=Kubernetes Kube-Proxy ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/proxyExecStart=/usr/local/bin/kube-proxy \\ $KUBE_LOGTOSTDERR \\ $KUBE_LOG_LEVEL \\ $KUBE_MASTER \\ $KUBE_PROXY_ARGSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target kube-proxy配置文件/etc/kubernetes/proxy 123456789cat &gt; /etc/kubernetes/proxy &lt;&lt;EOF#### kubernetes proxy config# default config should be adequate# Add your own!KUBE_PROXY_ARGS=\"--bind-address=192.168.10.57 --hostname-override=192.168.10.57 --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig --cluster-cidr=10.254.0.0/16\"EOF --hostname-override 参数值必须与 kubelet 的值一致，否则 kube-proxy 启动后会找不到该 Node，从而不会创建任何 iptables 规则 启动kube-proxy1234systemctl daemon-reloadsystemctl enable kube-proxysystemctl start kube-proxysystemctl status kube-proxy 安装dashboard参考: https://zhangkesheng.github.io/2017/11/16/Kubernetes-Install/#Kebernetes-Dashboard 此方法采用二进制安装, 存在升级问题等一些问题, 下一篇会讲解如何用docker-compose的方式安装 未完待续…","tags":[{"name":"笔记","slug":"笔记","permalink":"http://localhost:4000/tags/笔记/"}]},{"title":"Kubernetes Install","date":"2017-11-16T03:12:16.000Z","path":"2017/11/16/Kubernetes-Install/","text":"安装 Kubernetes准备工作 科学上网 关闭swap, swapoff -a 重启之后又会打开, 所以可以直接修改’/etc/fstab’. Installing DockerK8s(即’Kebunetes’) 1.8版本, 官方网站要安装说明安装了docker17.03, 若是docker版本过高, 也会有docker版本过高的提示, 所以这里就按照官网要求, 安装docker 17.03. 123456apt-get update &amp;&amp; apt-get install -y curl apt-transport-httpscurl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/docker.listdeb https://download.docker.com/linux/$(lsb_release -si | tr '[:upper:]' '[:lower:]') $(lsb_release -cs) stableEOFapt-get update &amp;&amp; apt-get install -y docker-ce=$(apt-cache madison docker-ce | grep 17.03 | head -1 | awk '&#123;print $3&#125;') Installing KubeadmKubernetes and docker 需要科学上网, 所以需要设置代理. docker代理, kubeadm init时需要下载镜像, 所以需要设置docker的代理 /etc/systemd/system/docker.service.d/http-proxy.conf 1234[Service]Environment=\"HTTP_PROXY=http://192.168.32.10:6780\"Environment=\"HTTPS_PROXY=http://192.168.32.10:6780\"Environment=\"NO_PROXY=.aliyun.com,.aliyuncs.com,.daocloud.io,.cn,localhost\" note: no_proxy不支持通配符 命令行代理 123export http_proxy=http://host:portexport https_proxy=http://host:portexport NO_PROXY=.aliyun.com,.aliyuncs.com,.daocloud.io,.cn,localhost,自己的IP #设置no_proxy, 说明见下面 install kubeadm 1234567apt-get update &amp;&amp; apt-get install -y apt-transport-httpscurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.listdeb http://apt.kubernetes.io/ kubernetes-xenial mainEOFapt-get updateapt-get install -y kubelet kubeadm kubectl Kueadm Init如果没有设置命令行no_proxy, 直接执行, 会结果如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.[init] Using Kubernetes version: v1.8.3[init] Using Authorization modes: [Node RBAC][preflight] Running pre-flight checks[preflight] WARNING: Connection to \"https://192.168.10.57:6443\" uses proxy \"http://192.168.32.10:6780\". If that is not intended, adjust your proxy settings[preflight] WARNING: Running with swap on is not supported. Please disable swap or set kubelet's --fail-swap-on flag to false.[kubeadm] WARNING: starting in 1.8, tokens expire after 24 hours by default (if you require a non-expiring token use --token-ttl 0)[certificates] Generated ca certificate and key.[certificates] Generated apiserver certificate and key.[certificates] apiserver serving cert is signed for DNS names [docker-host kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.10.57][certificates] Generated apiserver-kubelet-client certificate and key.[certificates] Generated sa key and public key.[certificates] Generated front-proxy-ca certificate and key.[certificates] Generated front-proxy-client certificate and key.[certificates] Valid certificates and keys now exist in \"/etc/kubernetes/pki\"[kubeconfig] Wrote KubeConfig file to disk: \"admin.conf\"[kubeconfig] Wrote KubeConfig file to disk: \"kubelet.conf\"[kubeconfig] Wrote KubeConfig file to disk: \"controller-manager.conf\"[kubeconfig] Wrote KubeConfig file to disk: \"scheduler.conf\"[controlplane] Wrote Static Pod manifest for component kube-apiserver to \"/etc/kubernetes/manifests/kube-apiserver.yaml\"[controlplane] Wrote Static Pod manifest for component kube-controller-manager to \"/etc/kubernetes/manifests/kube-controller-manager.yaml\"[controlplane] Wrote Static Pod manifest for component kube-scheduler to \"/etc/kubernetes/manifests/kube-scheduler.yaml\"[etcd] Wrote Static Pod manifest for a local etcd instance to \"/etc/kubernetes/manifests/etcd.yaml\"[init] Waiting for the kubelet to boot up the control plane as Static Pods from directory \"/etc/kubernetes/manifests\"[init] This often takes around a minute; or longer if the control plane images have to be pulled. #时间会很长, 要等一会[kubelet-check] It seems like the kubelet isn't running or healthy.[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10255/healthz' failed with error: Get http://localhost:10255/healthz: dial tcp [::1]:10255: getsockopt: connection refused.[kubelet-check] It seems like the kubelet isn't running or healthy.[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10255/healthz' failed with error: Get http://localhost:10255/healthz: dial tcp [::1]:10255: getsockopt: connection refused.[kubelet-check] It seems like the kubelet isn't running or healthy.[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10255/healthz' failed with error: Get http://localhost:10255/healthz: dial tcp [::1]:10255: getsockopt: connection refused.[kubelet-check] It seems like the kubelet isn't running or healthy.[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10255/healthz/syncloop' failed with error: Get http://localhost:10255/healthz/syncloop: dial tcp [::1]:10255: getsockopt: connection refused.[kubelet-check] It seems like the kubelet isn't running or healthy.[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10255/healthz/syncloop' failed with error: Get http://localhost:10255/healthz/syncloop: dial tcp [::1]:10255: getsockopt: connection refused.[kubelet-check] It seems like the kubelet isn't running or healthy.[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10255/healthz/syncloop' failed with error: Get http://localhost:10255/healthz/syncloop: dial tcp [::1]:10255: getsockopt: connection refused.[kubelet-check] It seems like the kubelet isn't running or healthy.[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10255/healthz' failed with error: Get http://localhost:10255/healthz: dial tcp [::1]:10255: getsockopt: connection refused.[kubelet-check] It seems like the kubelet isn't running or healthy.[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10255/healthz/syncloop' failed with error: Get http://localhost:10255/healthz/syncloop: dial tcp [::1]:10255: getsockopt: connection refused.[kubelet-check] It seems like the kubelet isn't running or healthy.[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10255/healthz' failed with error: Get http://localhost:10255/healthz: dial tcp [::1]:10255: getsockopt: connection refused.Unfortunately, an error has occurred: timed out waiting for the conditionThis error is likely caused by that: - The kubelet is not running - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled) - There is no internet connection; so the kubelet can't pull the following control plane images: - gcr.io/google_containers/kube-apiserver-amd64:v1.8.3 - gcr.io/google_containers/kube-controller-manager-amd64:v1.8.3 - gcr.io/google_containers/kube-scheduler-amd64:v1.8.3You can troubleshoot this for example with the following commands if you're on a systemd-powered system: - 'systemctl status kubelet' - 'journalctl -xeu kubelet'couldn't initialize a Kubernetes cluster 这里需要设置命令行的no_proxy, 也可以在设置代理的时候 1export NO_PROXY=.aliyun.com,.aliyuncs.com,.daocloud.io,.cn,localhost,自己的IP #设置no_proxy NOTE: 如果init的时间很长, 并且docker并没有什么反应, 可以重新删除k8s, 重新install再init. kubeadm init 没有使用代理则不会成功, 被坑惨了. 之后重新install才解决问题. 只了解了问题的表象, 需要去分析问题发生的原因. 成功结果如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.[init] Using Kubernetes version: v1.8.3[init] Using Authorization modes: [Node RBAC][preflight] Running pre-flight checks[kubeadm] WARNING: starting in 1.8, tokens expire after 24 hours by default (if you require a non-expiring token use --token-ttl 0)[certificates] Generated ca certificate and key.[certificates] Generated apiserver certificate and key.[certificates] apiserver serving cert is signed for DNS names [docker-host kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.10.57][certificates] Generated apiserver-kubelet-client certificate and key.[certificates] Generated sa key and public key.[certificates] Generated front-proxy-ca certificate and key.[certificates] Generated front-proxy-client certificate and key.[certificates] Valid certificates and keys now exist in &quot;/etc/kubernetes/pki&quot;[kubeconfig] Wrote KubeConfig file to disk: &quot;admin.conf&quot;[kubeconfig] Wrote KubeConfig file to disk: &quot;kubelet.conf&quot;[kubeconfig] Wrote KubeConfig file to disk: &quot;controller-manager.conf&quot;[kubeconfig] Wrote KubeConfig file to disk: &quot;scheduler.conf&quot;[controlplane] Wrote Static Pod manifest for component kube-apiserver to &quot;/etc/kubernetes/manifests/kube-apiserver.yaml&quot;[controlplane] Wrote Static Pod manifest for component kube-controller-manager to &quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&quot;[controlplane] Wrote Static Pod manifest for component kube-scheduler to &quot;/etc/kubernetes/manifests/kube-scheduler.yaml&quot;[etcd] Wrote Static Pod manifest for a local etcd instance to &quot;/etc/kubernetes/manifests/etcd.yaml&quot;[init] Waiting for the kubelet to boot up the control plane as Static Pods from directory &quot;/etc/kubernetes/manifests&quot;[init] This often takes around a minute; or longer if the control plane images have to be pulled.[apiclient] All control plane components are healthy after 156.501511 seconds[uploadconfig] Storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace[markmaster] Will mark node docker-host as master by adding a label and a taint[markmaster] Master docker-host tainted and labelled with key/value: node-role.kubernetes.io/master=&quot;&quot;[bootstraptoken] Using token: ce5ad4.07b7b63c25b711b0[bootstraptoken] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials[bootstraptoken] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token[bootstraptoken] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster[bootstraptoken] Creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace[addons] Applied essential addon: kube-dns[addons] Applied essential addon: kube-proxyYour Kubernetes master has initialized successfully!To start using your cluster, you need to run (as a regular user): mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at: http://kubernetes.io/docs/admin/addons/You can now join any number of machines by running the following on each nodeas root: kubeadm join --token ce5ad4.07b7b63c25b711b0 192.168.10.57:6443 --discovery-token-ca-cert-hash sha256:da4ea96d2fdb7c3936d0870534688926e6c47738eca6c1d3b74283bf4ec0f171 docker ps 查看k8s运行情况 为让Pod直接可以相互通信, 需要安装网络插件, k8s支持很多种网络, 包括Weave, Calico, Flannel等, 详情见https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/. 我们选择了Weave Net, 安装命令 1kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\" kubernetes配置 1echo \"export KUBECONFIG=/etc/kubernetes/admin.conf\" &gt;&gt; ~/.bashrc 之后就可以执行kubectl get nodes 查看node 如果status是NotReady, 需要等一下, docker container全部运行成功后status就会变成Ready. Kebernetes Dashboard安装k8s dashboard github 地址 kubernetes/dashboard 123git clone https://github.com/kubernetes/heapster.gitkubectl create -f deploy/kube-config/influxdb/kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml 然后执行kubectl -n kube-system edit service kubernetes-dashboard 将spec.type改为NodePort, 添加spec.porte.nodePort=31700, 31700为访问dashboard的IP. 之后浏览器访问https://:31700就可以查看dashboard. 测试环境skip跳过登录就可以查看, 会有权限问题. 添加一个dashboard-admin.yaml文件, 内容如下: 1234567891011121314apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: kubernetes-dashboard labels: k8s-app: kubernetes-dashboardroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: kubernetes-dashboard namespace: kube-system 执行kubectl create -f dashboard-admin.yaml就可以不用登录查看dashboard. NOTE: 测试时可以这样, 线上千万不要这样做! 到此, 就安装好了k8s, 并且添加了dashboard查看相关的内容. k8s更多的内容就要各位自己去发现了. 未完待续…","tags":[{"name":"笔记","slug":"笔记","permalink":"http://localhost:4000/tags/笔记/"}]},{"title":"Hello World","date":"2017-08-23T07:35:42.000Z","path":"2017/08/23/hello-world/","text":"Hello World!","tags":[]},{"title":"第一篇博客","date":"2017-08-23T07:35:42.000Z","path":"2017/08/23/first/","text":"第一篇任性的博客, 啥都不写","tags":[{"name":"无题","slug":"无题","permalink":"http://localhost:4000/tags/无题/"}]}]